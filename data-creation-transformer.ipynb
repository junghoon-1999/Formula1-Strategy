{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries for the data creation step of the project\n",
    "import fastf1 as f1\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Data creation and engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1-1. Data Creation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The primary data source is from the FastF1 library (https://github.com/theOehrly/Fast-F1). We will be using the telemetry data from the 2023 season to predict change of positions for all drivers. The justification for the use of data from 2023 has been specified in the data-analysis.ipynb file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select all race events in 2023\n",
    "Events = f1.get_event_schedule(2023)\n",
    "Events_Race = Events[Events['Session5'] == 'Race']\n",
    "total_tele = pd.DataFrame()\n",
    "Event = Events_Race.loc[1, :]\n",
    "\n",
    "# Load session object\n",
    "session = f1.core.Session(Event, session_name = 'Race', f1_api_support = True)\n",
    "session.load(laps = True, telemetry = True, weather = True, messages = True)\n",
    "\n",
    "# Load laps and results data\n",
    "sesh_l = session.laps\n",
    "sesh_r = session.results\n",
    "\n",
    "# Attain all the drivers from the lap\n",
    "drivers = list(sesh_l['Driver'].unique())\n",
    "\n",
    "for drv in drivers:\n",
    "    total_drv = pd.DataFrame()\n",
    "\n",
    "    # Total number of laps the driver had \n",
    "    total_laps = int(sesh_l.pick_driver(drv).LapNumber.iloc[-1])\n",
    "    \n",
    "    distance = 0\n",
    "\n",
    "    for j in range(total_laps):\n",
    "\n",
    "        temp_tele = sesh_l.pick_driver(drv).iloc[j].get_telemetry().add_distance()\n",
    "        temp_tele['Brake'] = temp_tele['Brake'].astype(int)\n",
    "\n",
    "        # Adding data from session.Laps\n",
    "        # Laps: which lap the driver is in\n",
    "        # Compound: Which compound it's in\n",
    "        # TyreLife: How long the Tyre has been used \n",
    "        # TrackStatus: What the track status is \n",
    "        temp_tele['Lap'] = j+1\n",
    "        temp_tele['Compound'] = sesh_l.pick_driver(drv).iloc[j]['Compound']\n",
    "        temp_tele['TyreLife'] = sesh_l.pick_driver(drv).iloc[j]['TyreLife']\n",
    "        temp_tele['TrackStatus'] = sesh_l.pick_driver(drv).iloc[j]['TrackStatus']\n",
    "\n",
    "        # Combining the dataset \n",
    "        total_drv = pd.concat([total_drv, temp_tele.reset_index(drop=True)], axis = 0)\n",
    "\n",
    "    # Drop columns we don't need \n",
    "    total_drv.drop(columns = ['Time', 'Source', 'DriverAhead', 'DistanceToDriverAhead'], inplace = True)\n",
    "\n",
    "    # Add a status column for each telemetry input\n",
    "    outcome = sesh_r[sesh_r['Abbreviation'] == drv]['Status'].values[0]\n",
    "\n",
    "    if outcome == 'Finished':\n",
    "        total_drv['Status'] = 'Finished'\n",
    "    \n",
    "    elif '+' in outcome:\n",
    "        total_drv['Status'] = 'Finished'\n",
    "        total_drv['Status'].iloc[-1] = 'Lapped'\n",
    "    \n",
    "    else:\n",
    "        total_drv['Status'] = 'Finished'\n",
    "        total_drv['Status'].iloc[-1] = 'DNF'\n",
    "\n",
    "    for i in total_drv.columns:\n",
    "        new_col = drv + '_' + i\n",
    "        total_drv.rename(columns = {i: new_col}, inplace = True)\n",
    "\n",
    "    # Concatenate all the data from a single race together\n",
    "    total_tele = pd.concat([total_tele, total_drv.reset_index(drop = True)], axis = 1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is consisted of the following columns for each driver \n",
    "\n",
    "| Feature | Type | Description |\n",
    "| --- | --- | --- |\n",
    "| Date | TimeDelta | The timestamp of when the data was collected |\n",
    "| SessionDate | TimeDelta | The relative timestamp of the session |\n",
    "| RPM | int | The RPM of the vehicle |\n",
    "| Speed | int | The speed of the vehicle |\n",
    "| nGear | int | The gear status of the vehicle |\n",
    "| Throttle | int | The % of throttle pressure |\n",
    "| Brake | Bool | The brake status |\n",
    "| DRS | Bool | The DRS status |\n",
    "| RelativeDistance | int | Distance driven since first sample |\n",
    "| X | int | X position (1/10 m) |\n",
    "| Y | int | Y position (1/10 m) |\n",
    "| Z | int | Z position (1/10 m) |\n",
    "| Status | Cat (str) | Current status of the driver (DNF, Finished etc) |\n",
    "| TrackStatus | Cat (str) | Flag (Yellow flag, Safety Car, Red Flag, Virtual Safety Car) | \n",
    "| Compound | Cat (str)|The Tyre Compound (Soft, Medium, Hard, Intermediate, Wet) |\n",
    "| PitIn | Bool | Driver pit in status |\n",
    "| PitOut | Bool | Driver pit out status |\n",
    "| Distance | int | The total distance driven for the lap |\n",
    "| Corner | int| The distance to the nearest turn |\n",
    "| Angle | Cat (str) | The severity of the turn divided into 4 classes (Low (0-45), Med-Low (45-90), Med-High (90-120), High (120-180)) |\n",
    "\n",
    "Here the categorical and boolean values will seperately be encoded as Dummy Variables. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the Date and Session Time column with the most input amongst drivers and use that\n",
    "# SessionTime is kept to merge weather data \n",
    "def mx_len(df, col_name):\n",
    "    mx_len = 0\n",
    "\n",
    "    for col in df.columns:\n",
    "        if col_name in col:\n",
    "            if total_tele[col].count() > mx_len:\n",
    "\n",
    "                if mx_len != 0:\n",
    "                    df.drop(columns = col_name, inplace = True)\n",
    "                mx_len = max(mx_len, df[col].count())\n",
    "                df.rename(columns = {col: col_name}, inplace = True)\n",
    "            else:\n",
    "                df.drop(columns = col, inplace = True)\n",
    "    df = df[[col_name] + [col for col in df.columns if col != col_name]]\n",
    "    return df\n",
    "\n",
    "total_tele = mx_len(total_tele, 'Date')\n",
    "total_tele = mx_len(total_tele, 'SessionTime')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will also add the weather data of the specified timestamp from the FastF1 library. The weather_data is a telemetry data with specified descriptions as show below, all of which will be included into our data.\n",
    "\n",
    "| Feature | Type | Description |\n",
    "| --- | --- | --- |\n",
    "| AirTemp | Int | Temperature |\n",
    "| Humidity | Int | Humidity |\n",
    "| Pressure | Int | Air pressure|\n",
    "| RainFall | bool | Show if there is rainfall |\n",
    "| TrackTemp | Int | Temperature of the track |\n",
    "| WindDirection | Int | Direction of the wind |\n",
    "| WindSpeed | Int | Speed of the wind | "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add weatherdata \n",
    "weather_data = session.weather_data\n",
    "weather_data['Time'] = pd.to_timedelta(weather_data['Time'])\n",
    "weather_data['Rainfall'] = weather_data['Rainfall'].astype(int)\n",
    "\n",
    "# Add the weather data df to the total_tele df and drop SessionTime as it is no longer needed\n",
    "total_tele = pd.merge_asof(total_tele, weather_data, left_on = 'SessionTime', right_on = 'Time', direction = 'nearest')\n",
    "total_tele.drop(columns=['Time'], inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1-2. Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, our existing data will undergo feature engineering for the model (Transformer) of our choice. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1-2-1. Distance "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Re-scale the values of the 'Distance' columns such that they are all within the range (0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Range \n",
    "for i in drivers:\n",
    "    total_tele[i + '_Distance'] /= 5412"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1-2-2. TrackStatus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Engineer TrackStatus to attain just the crucial part of the track\n",
    "# Define function to return the largest integer value within the track status input\n",
    "def return_max(col):\n",
    "    if isinstance(col, float): return col\n",
    "    else: \n",
    "        col = max(list(col))\n",
    "    return col\n",
    "\n",
    "for i in total_tele.columns:\n",
    "    if 'TrackStatus' in i:\n",
    "        total_tele[i] = total_tele[i].apply(return_max)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1-2-3. Imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imputation for continuous features\n",
    "cts = ['RPM', 'Speed', 'nGear', 'Brake', 'Throttle', 'DRS', 'X', 'Y', 'Z', 'TyreLife', 'Distance', '_Corner']\n",
    "for cols in cts:\n",
    "    for drv in drivers:\n",
    "        total_tele[drv + '_' + cols].fillna(0, inplace = True)\n",
    "\n",
    "# Imputation for the compound column\n",
    "for drv in drivers:\n",
    "    total_tele[drv + '_' + 'Compound'].fillna('Done', inplace = True)\n",
    "\n",
    "# Imputation for the status column\n",
    "for drv in drivers:\n",
    "    i = drv + '_' + 'Status'\n",
    "    final_rec = list(total_tele[total_tele[i].isna() == False][i])[-1]\n",
    "    total_tele[i].fillna(final_rec, inplace = True)\n",
    "\n",
    "# Imputing the angle column\n",
    "for drv in drivers:\n",
    "    total_tele[drv + '_Angle'].fillna('Done', inplace = True)\n",
    "\n",
    "# Imputing the Lap and TrackStatus columns\n",
    "def impute(col, val):\n",
    "    for drv in drivers:\n",
    "        if sesh_r[sesh_r['Abbreviation'] == drv]['Status'].values[0] == 'Finished':\n",
    "            i = drv + '_' + col\n",
    "            total_tele[i].fillna(val, inplace = True)\n",
    "            # total_tele[i].fillna(-1, inplace = True)\n",
    "        else:\n",
    "            i = drv + '_' + col\n",
    "            final_rec = list(total_tele[total_tele[i].isna() == False][i])[-1]\n",
    "            total_tele[i].fillna(final_rec, inplace = True)\n",
    "\n",
    "# imputing Acc_Distance\n",
    "def impute_acc_dist():\n",
    "    for drv in drivers:\n",
    "        i = drv + '_' + 'Acc_Distance'\n",
    "        final_rec = list(total_tele[total_tele[i].isna() == False][i])[-1]\n",
    "        total_tele[i].fillna(final_rec, inplace = True)\n",
    "            \n",
    "impute('Lap', 57)\n",
    "impute('TrackStatus', '9')\n",
    "impute_acc_dist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1-2-4. Dummy Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the categorical columns as dummy variables\n",
    "cols = []\n",
    "for drv in drivers:\n",
    "    col_1 = drv +'_Status'\n",
    "    col_2 = drv + '_TrackStatus'\n",
    "    col_3 = drv + '_Compound'\n",
    "    col_4 = drv + '_nGear'\n",
    "    col_5 = drv + '_Angle'\n",
    "    cols.append(col_1)\n",
    "    cols.append(col_2)\n",
    "    cols.append(col_3)\n",
    "    cols.append(col_4)\n",
    "    cols.append(col_5)\n",
    "    total_tele[col_1] = pd.Categorical(total_tele[col_1], categories = ['Finished', 'DNF', 'Lapped'])\n",
    "    total_tele[col_2] = pd.Categorical(total_tele[col_2], categories = ['1', '2', '4', '5', '6', '7', '8', '0'])\n",
    "    total_tele[col_3] = pd.Categorical(total_tele[col_3], categories = ['SOFT', 'MEDIUM', 'HARD', 'INTERMEDIATE', 'WET', 'DONE'])\n",
    "    ## the '9' represents the chequered flag for the driver\n",
    "    total_tele[col_4] = pd.Categorical(total_tele[col_4], categories = ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9'])\n",
    "    total_tele[col_5] = pd.Categorical(total_tele[col_5], categories = ['Low', 'Med-Low', 'Med-High', 'High', 'Done'])\n",
    "temp = pd.get_dummies(total_tele[cols], dtype = int)\n",
    "\n",
    "total_tele.drop(columns = cols, inplace = True)\n",
    "total_tele = pd.concat([total_tele, temp], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1-2-5. Pit_in & Pit_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for drv in drivers:\n",
    "    sesh = sesh_l.pick_drivers(drv)\n",
    "    sesh['PitInTime'] = pd.to_timedelta(sesh['PitInTime'])\n",
    "    sesh['PitOutTime'] = pd.to_timedelta(sesh['PitOutTime'])\n",
    "    In_df = sesh[sesh['PitInTime'].isna() == False]['PitInTime'].to_frame()\n",
    "    Out_df = sesh[sesh['PitOutTime'].isna() == False]['PitOutTime'].to_frame()\n",
    "\n",
    "    temp = pd.to_timedelta(total_tele['SessionTime'])\n",
    "\n",
    "    in_temp = pd.merge_asof(In_df, temp, left_on = 'PitInTime', right_on = 'SessionTime', direction = 'nearest').SessionTime\n",
    "    out_temp = pd.merge_asof(Out_df, temp, left_on = 'PitOutTime', right_on = 'SessionTime', direction = 'nearest').SessionTime\n",
    "\n",
    "    total_tele[drv + '_Pit'] = 0\n",
    "\n",
    "    if len(in_temp) == len(out_temp):\n",
    "        for pit_in, pit_out in zip(in_temp, out_temp):\n",
    "            total_tele[drv + '_Pit'].loc[(pit_in <= total_tele['SessionTime'])&(total_tele['SessionTime'] <= pit_out)] = 1\n",
    "    else:\n",
    "        for i in range(len(out_temp)):\n",
    "            total_tele[drv + '_Pit'].loc[(in_temp.loc[i]  <= total_tele['SessionTime'])&(total_tele['SessionTime'] <= out_temp.loc[i])] = 1\n",
    "        total_tele[drv + '_Pit'].loc[(in_temp.loc[len(in_temp)-1] == total_tele['SessionTime'])] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1-2-6. Scaling the Integer Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attain the max value for the columns: RPM, Speed, Throttle, Distance\n",
    "def id_max(col_name):\n",
    "    max_val = int()\n",
    "    for i in drivers:\n",
    "        max_val = max(max_val, max(total_tele[i + '_'+ col_name]))\n",
    "    return max_val \n",
    "\n",
    "info = dict()\n",
    "for i in ['RPM', 'Speed', 'Throttle']:\n",
    "    info[i] = id_max(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for drv in drivers:\n",
    "    for cols in info.keys():\n",
    "        total_tele[drv + '_' + cols] /= info[cols]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1-3. Wrap Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop SessionTime and Date column as it is no longer needed\n",
    "total_tele.drop(columns = ['SessionTime', 'Date'], inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in drivers:\n",
    "    total_tele.drop(columns = [i + '_RelativeDistance'], inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_tele[(total_tele['VER_Distance'] != 0)]['VER_Distance']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Sequence and Target Generation (Data Wrangling)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section of the code will generate sequences and targets using the data that we've just created. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sequences and labels for the data \n",
    "seq = []\n",
    "lab = []\n",
    "target = []\n",
    "remainder = []\n",
    "start_seq = 0\n",
    "end_seq = 0\n",
    "drivers_distance = [i+'_Distance' for i in drivers]\n",
    "distances = total_tele[drivers_distance]\n",
    "for i in range(50, len(total_tele), 50):\n",
    "    end_seq = i\n",
    "\n",
    "    # Append 50 items to the sequence list and convert them to numpy\n",
    "    seq.append(total_tele.loc[start_seq:end_seq-1].to_numpy())\n",
    "\n",
    "    # Append the last item of every subsequent 50 items to the target list and convert them to numpy\n",
    "    target.append(distances.loc[start_seq:end_seq-1].to_numpy())\n",
    "    start_seq = i\n",
    "    #lab.append(total_tele.loc[end_seq:end_seq+10].to_numpy())\n",
    "seq = np.array(seq)[:-1]\n",
    "target = np.array(target)[1:]\n",
    "#lab = np.array(lab)\n",
    "remainder.append(total_tele.loc[len(total_tele) - len(total_tele)%50+1: len(total_tele)].to_numpy())\n",
    "seq = torch.stack([torch.tensor(s.astype(float), dtype=torch.float) for s in seq])\n",
    "target = torch.stack([torch.tensor(s.astype(float), dtype=torch.float) for s in target])\n",
    "# target = target/57"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create training and test set \n",
    "# Shuffling the sequence and target dataset\n",
    "np.random.seed(12)\n",
    "shuffled_indices = np.random.permutation(len(seq))\n",
    "seq = seq[shuffled_indices]\n",
    "target = target[shuffled_indices]\n",
    "\n",
    "# Define testing dataset and training dataset \n",
    "train_size = int(len(seq)*0.8)\n",
    "val_size = int(len(seq)*0.9)\n",
    "\n",
    "train_seq = seq[:train_size]\n",
    "train_target = target[:train_size]\n",
    "\n",
    "val_seq = seq[train_size:val_size]\n",
    "val_target = target[train_size:val_size]\n",
    "\n",
    "test_seq = seq[val_size:]\n",
    "test_target = target[val_size:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "bs = 16\n",
    "\n",
    "# Load and preprocess data for deep learning models\n",
    "input_dataloader = DataLoader(train_seq, batch_size=bs)\n",
    "target_dataloader = DataLoader(train_target, batch_size=bs)\n",
    "\n",
    "test_input_dataloader = DataLoader(val_seq, batch_size=bs)\n",
    "test_target_dataloader = DataLoader(val_target, batch_size=bs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "realtest_input_dataloader = DataLoader(test_seq, batch_size=bs)\n",
    "realtest_target_dataloader = DataLoader(test_target, batch_size=bs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Setting up the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FFN(torch.nn.Module):\n",
    "    def __init__(self, hidden_dim):\n",
    "        super(FFN, self).__init__()\n",
    "        self.gelu = nn.GELU()\n",
    "        self.ffn = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.norm = nn.LayerNorm(hidden_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "        x = self.ffn(x)\n",
    "        x = self.gelu(x) + identity\n",
    "        x = self.norm(x)\n",
    "        return x\n",
    "\n",
    "class SelfAttnLayer(torch.nn.Module):\n",
    "    def __init__(self, hidden_dim, num_heads):\n",
    "        super(SelfAttnLayer, self).__init__()\n",
    "        self.gelu = nn.GELU()\n",
    "        self.mha = nn.MultiheadAttention(hidden_dim, num_heads, batch_first=True)\n",
    "        self.norm = nn.LayerNorm(hidden_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "        x, _ = self.mha(x, x, x)\n",
    "        x = self.gelu(x) + identity\n",
    "        x = self.norm(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Model(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, hidden_dim, num_heads, n_layers, rate):\n",
    "        super(Model, self).__init__()\n",
    "        self.linear_in = nn.Linear(927, hidden_dim)\n",
    "        self.gelu = nn.GELU()\n",
    "\n",
    "        module_lst = []\n",
    "        for _ in range(n_layers):\n",
    "            module_lst.append(SelfAttnLayer(hidden_dim, num_heads))\n",
    "            module_lst.append(FFN(hidden_dim))\n",
    "            module_lst.append(nn.Dropout(p = rate))\n",
    "\n",
    "        self.module_lst = nn.ModuleList(module_lst)\n",
    "\n",
    "        self.linear2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.out_head = nn.Linear(hidden_dim, 40)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear_in(x)\n",
    "        x = self.gelu(x)\n",
    "        for module in self.module_lst:\n",
    "            x = module(x)\n",
    "        x = self.out_head(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4-1. Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For Hyperparameter tuning we will be using the optuna library for the following parameters:\n",
    "\n",
    "| Hyperparameter | Range |\n",
    "| --- | --- |\n",
    "| hidden_dim | [256, 512, 768, 1024] |\n",
    "| num_heads | [2, 4, 8] |\n",
    "| n_layers | [1, 2, 3, 4] |\n",
    "| rate | [0.1, 0.75, 0.5] |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "  n_layers = trial.suggest_int(\"n_layers\", 1, 4)\n",
    "  hidden_dim = trial.suggest_int(\"hidden_dim\", 256, 1024, step = 256)\n",
    "  num_heads = trial.suggest_categorical(\"num_heads\",[2, 4, 8])\n",
    "  rate = trial.suggest_categorical('rate', [0.1, 0.75, 0.5])\n",
    "  #optimizer_name = trial.suggest_categorical(\"optimizer\", [\"Adam\", \"RMSprop\", \"SGD\"])\n",
    "  #lr = trial.suggest_float(\"lr\", 0.0001, 0.01)\n",
    "  #opimizer = getattr(optim, optimizer_name)(model.parameters(), lr = lr)\n",
    "  device = torch.device('cuda')\n",
    "  model = Model(hidden_dim, num_heads, n_layers, rate).to(device)\n",
    "  model = model.float()\n",
    "  loss = nn.L1Loss()\n",
    "  optimizer = torch.optim.AdamW(model.parameters(), lr = 0.0003) # 0.01, 0.005\n",
    "\n",
    "  for epoch in range(300):\n",
    "\n",
    "\n",
    "    model.train()\n",
    "    loss_val = 0\n",
    "\n",
    "    for s, t in zip(input_dataloader, target_dataloader):\n",
    "      s = s.to(device)\n",
    "      t = t.to(device)\n",
    "      optimizer.zero_grad()\n",
    "      output = model(s)\n",
    "      mae_loss = loss(output, t)\n",
    "      mae_loss.backward()\n",
    "      optimizer.step()\n",
    "\n",
    "    model.eval()\n",
    "    test_loss_val = 0\n",
    "    for s, t in zip(test_input_dataloader, test_target_dataloader):\n",
    "        s = s.to(device)\n",
    "        t = t.to(device)\n",
    "        # opt.zero_grad()\n",
    "        logits = model(s)\n",
    "\n",
    "        # outputs.append(logits)\n",
    "        mse_loss = loss(logits, t)\n",
    "        # mse_loss.backward()\n",
    "        params = []\n",
    "        #for name, param in model.named_parameters():\n",
    "        #    if param.requires_grad:\n",
    "        #        params.append([name, param.data])\n",
    "        # opt.step()\n",
    "\n",
    "        test_loss_val += (mse_loss.item())\n",
    "\n",
    "    test_avg_loss = test_loss_val / len(test_input_dataloader)\n",
    "\n",
    "    trial.report(test_avg_loss, epoch)\n",
    "    \n",
    "    if trial.should_prune():\n",
    "      raise optuna.exceptions.TrialPruned()\n",
    "\n",
    "  return test_avg_loss\n",
    "\n",
    "study = optuna.create_study(direction=\"minimize\")\n",
    "study.optimize(objective, n_trials=10)\n",
    "print('Best trial:')\n",
    "trial = study.best_trial\n",
    "\n",
    "print(\"value: {:.4f}\".format(trial.value))\n",
    "\n",
    "print(\"Params: \")\n",
    "for key, value in trial.params.items():\n",
    "  print(\"    {}: {}\".format(key, value))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4-2. Model Fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda')\n",
    "model = Model(1024, 4).to(device)\n",
    "loss = nn.MSELoss()\n",
    "opt = torch.optim.AdamW(model.parameters(), lr = 0.0003) # 0.01, 0.005\n",
    "opt_2 = torch.optim.SGD(model.parameters(), lr = 0.0003)\n",
    "# opt = torch.optim.RMSprop(model.parameters(), lr = 0.01)\n",
    "\n",
    "losses = []\n",
    "test_losses = []\n",
    "\n",
    "outputs = []\n",
    "tot = []\n",
    "min_val_loss = 1000000000000\n",
    "for epoch in range(500):\n",
    "    model.train()\n",
    "    loss_val = 0\n",
    "    for s, t in zip(input_dataloader, target_dataloader):\n",
    "        s = s.to(device)\n",
    "        t = t.to(device)\n",
    "        opt.zero_grad()\n",
    "        logits = model(s)\n",
    "\n",
    "        # outputs.append(logits)\n",
    "        mse_loss = loss(logits, t)\n",
    "        mse_loss.backward()\n",
    "        params = []\n",
    "        #for name, param in model.named_parameters():\n",
    "        #    if param.requires_grad:\n",
    "        #        params.append([name, param.data])\n",
    "        opt.step()\n",
    "\n",
    "        loss_val += (mse_loss.item())\n",
    "    # tot.append(params)\n",
    "    avg_loss = loss_val / len(input_dataloader)\n",
    "    losses.append(avg_loss)\n",
    "\n",
    "    model.eval()\n",
    "    test_loss_val = 0\n",
    "    for s, t in zip(test_input_dataloader, test_target_dataloader):\n",
    "        s = s.to(device)\n",
    "        t = t.to(device)\n",
    "        # opt.zero_grad()\n",
    "        logits = model(s)\n",
    "\n",
    "        # outputs.append(logits)\n",
    "        mse_loss = loss(logits, t)\n",
    "        # mse_loss.backward()\n",
    "        params = []\n",
    "        #for name, param in model.named_parameters():\n",
    "        #    if param.requires_grad:\n",
    "        #        params.append([name, param.data])\n",
    "        # opt.step()\n",
    "\n",
    "        test_loss_val += (mse_loss.item())\n",
    "    # tot.append(params)\n",
    "    test_avg_loss = test_loss_val / len(test_input_dataloader)\n",
    "    if test_avg_loss < min_val_loss:\n",
    "        min_val_loss = test_avg_loss\n",
    "        best_model = model.state_dict()\n",
    "\n",
    "    test_losses.append(test_avg_loss)    \n",
    "    print(f\"Epoch {epoch}, {avg_loss= }, {test_avg_loss= }\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4-3. Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(best_model)\n",
    "model.eval()\n",
    "test_loss_val = 0\n",
    "for s, t in zip(realtest_input_dataloader, realtest_target_dataloader):\n",
    "    s = s.to(device)\n",
    "    t = t.to(device)\n",
    "    logits = model(s)\n",
    "    mse_loss = loss(logits, t)\n",
    "    params = []\n",
    "    test_loss_val += (mse_loss.item())\n",
    "test_avg_loss = test_loss_val / len(realtest_input_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Result Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
